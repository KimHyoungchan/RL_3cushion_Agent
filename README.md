# RL for 3-Cushion Billiards (PPO)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ê°•í™”í•™ìŠµ **PPO (Proximal Policy Optimization)**ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬,  
ë¬¼ë¦¬ ì—”ì§„ ê¸°ë°˜ì˜ 3ì¿ ì…˜ ë‹¹êµ¬ í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ê°€ ë“ì  ê²½ë¡œë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ì—°êµ¬ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

## Project Overview

- **Goal**  
  - ì—°ì†ì ì¸ í–‰ë™ ê³µê°„(í˜, íƒ€ê²© ê°ë„, ë‹¹ì /íšŒì „ ë“±)ì—ì„œ  
    **3ì¿ ì…˜ ë“ì ì„ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ ì •ì±…(Policy)** ì„ í•™ìŠµí•˜ëŠ” ê²ƒ.
  - ì—ì´ì „íŠ¸ê°€ ì‚¬ëŒê³¼ ìœ ì‚¬í•œ ìƒ· ì„ íƒ(ì´ˆì´ìŠ¤)ê³¼ **ì—°ì† ë“ì (run length)** ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ”ì§€ ë¶„ì„.

- **Environment**
  - **PyGame & Pymunk** ê¸°ë°˜ì˜ ì»¤ìŠ¤í…€ ë‹¹êµ¬ ì‹œë®¬ë ˆì´í„°
  - 3ê°œì˜ ê³µ(ìˆ˜êµ¬ + ì êµ¬ 2ê°œ)ì™€ í…Œì´ë¸”, ì¿ ì…˜, ë§ˆì°°, ë°˜ë°œë ¥ ë“±ì„ ê·¼ì‚¬í•œ 2D ë¬¼ë¦¬ í™˜ê²½
  - Gymnasium ìŠ¤íƒ€ì¼ API (`reset()`, `step()`, `render()`)

- **Algorithm**
  - **Stable Baselines 3 (SB3)** ì˜ **PPO ì—ì´ì „íŠ¸**
  - ì—°ì† í–‰ë™ ê³µê°„ì„ ìœ„í•œ Gaussian ì •ì±… + Tanh squash êµ¬ì¡°

- **Key Challenges**
  - **Sparse Reward (í¬ì†Œ ë³´ìƒ)**  
    - 3ì¿ ì…˜ ë“ì ì´ë¼ëŠ” ì´ë²¤íŠ¸ê°€ ë“œë¬¼ê²Œ ë°œìƒ â†’ í•™ìŠµ ì´ˆê¸° ì‹ í˜¸ ë¶€ì¡±
  - **Policy Collapse (ì •ì±… ë¶•ê´´)**  
    - í•™ìŠµ í›„ë°˜ë¶€ì— íŠ¹ì • í–‰ë™ì— ê³¼ë„í•˜ê²Œ ìˆ˜ë ´ â†’ ì„±ëŠ¥ ê¸‰ë½ í˜„ìƒ ë°©ì§€/ë³µêµ¬ê°€ í•„ìš”

---

## Tech Stack

- **Language**
  - Python 3.12

- **RL / DL**
  - [Stable Baselines 3](https://github.com/DLR-RM/stable-baselines3)
  - PyTorch

- **Physics / Rendering**
  - PyGame
  - Pymunk

- **Logging / Visualization**
  - Matplotlib
  - TensorBoard (ì„ íƒ)

---

## Installation

### 1. Repository Clone

```bash
git clone https://github.com/KimHyoungchan/RL_3cushion_agent.git
cd billiard-rl-agent
```

### 2. Dependencies ì„¤ì¹˜

ê°€ìƒí™˜ê²½ ì‚¬ìš©

```bash
pip install -r requirements.txt
```

`requirements.txt`ê°€ ì—†ë‹¤ë©´ ê¸°ë³¸ ì˜ì¡´ì„±ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```bash
pip install   gymnasium   stable-baselines3   shimmy   pygame   pymunk   matplotlib   tensorboard
```

---

## Usage

### 1. Training (From Scratch)

ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•  ë•Œ:

- `play.py` ë‚´ì—ì„œ `TRAIN_MODE = True` ë¡œ ì„¤ì •í•˜ê±°ë‚˜,
- CLI ì¸ìë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:

```bash
python main.py --train
```

ì˜ˆì‹œ (ë‚´ë¶€ ë¡œì§ ê¸°ì¤€):

```python
if __name__ == "__main__":
    from stable_baselines3 import PPO
    from environment_set import BilliardEnv
    from stable_baselines3.common.vec_env import DummyVecEnv

    env = DummyVecEnv([lambda: BilliardEnv(render_mode=None)])

    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ (ì˜ˆì‹œ)
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
    )

    model.learn(total_timesteps=500_000)
    model.save("logs/ppo_billiards_3cushion")
```

---

### 2. Resume Training (Manual Fine-tuning)

**í•µì‹¬ ê¸°ëŠ¥**:  
í•™ìŠµ ë„ì¤‘ ë˜ëŠ” ì¥ì‹œê°„ í•™ìŠµ í›„ ì„±ëŠ¥ì´ ë–¨ì–´ì¡Œì„ ë•Œ,  
íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•´ì„œ **í•™ìŠµë¥ (LR) ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì • í›„ ì¬í•™ìŠµ**í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.  
ì´ëŠ” íŠ¹íˆ **Policy Collapse(ì •ì±… ë¶•ê´´)** ë°œìƒ ì‹œ ì •ì±…ì„ ë³µêµ¬í•˜ê±°ë‚˜ ë” ì•ˆì •ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

```python
from stable_baselines3 import PPO
from environment_set import BilliardEnv
from stable_baselines3.common.vec_env import DummyVecEnv

env = DummyVecEnv([lambda: BilliardEnv(render_mode=None)])

# ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model = PPO.load("logs/best_model.zip", env=env)

# ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°•ì œ ì£¼ì… (Fine-tuning)
NEW_LR = 3e-5  # ê¸°ì¡´ 3e-4 â†’ 1/10ë¡œ ê°ì†Œ
for param_group in model.policy.optimizer.param_groups:
    param_group["lr"] = NEW_LR

# ì¶”ê°€ í•™ìŠµ
model.learn(total_timesteps=100_000)
model.save("logs/ppo_billiards_3cushion_finetuned")
```

---

### 3. Watching / Evaluation (Policy í…ŒìŠ¤íŠ¸)

í•™ìŠµëœ ì •ì±…ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì¹˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸:

```bash
python main.py --watch
```

ì˜ˆì‹œ ì½”ë“œ êµ¬ì¡°:

```python
from stable_baselines3 import PPO
from environment_set import BilliardEnv
from stable_baselines3.common.vec_env import DummyVecEnv

env = DummyVecEnv([lambda: BilliardEnv(render_mode="human")])
model = PPO.load("logs/ppo_billiards_3cushion", env=env)

obs, _ = env.reset()
for step in range(200):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated:
        obs, _ = env.reset()
```

---

## Key Features & Techniques

### 1. Reward Normalization (Custom Scaling)

> í˜„ì¬ êµ¬í˜„ì€ **í™˜ê²½ ë‚´ë¶€ì—ì„œ ì§ì ‘ ë³´ìƒì„ ìŠ¤ì¼€ì¼ë§**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.  
> (VecNormalizeëŠ” í•´ì œ, í•„ìš”ì‹œ ì„ íƒì ìœ¼ë¡œ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)

- **ëª©ì **
  - ë³´ìƒ ìŠ¤ì¼€ì¼(ì˜ˆ: ë¯¸ìŠ¤ -7, ì„±ê³µ +50)ì˜ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´  
    Critic lossê°€ ì‰½ê²Œ í­ë°œí•˜ê±°ë‚˜ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆìŒ.
- **ë°©ë²•**
  - í™˜ê²½ ë‚´ë¶€ì—ì„œ `_calculate_reward()`ë¡œ **ì›ì‹œ ë³´ìƒ(raw_reward)** ë¥¼ ê³„ì‚°  
    (3ì¿ ì…˜ ì„±ê³µ, íŒŒìš¸, ê±°ë¦¬ ê°œì„  ë“±)
  - `step()`ì—ì„œ:
    ```python
    raw_reward = self._calculate_reward(...)
    normalized_reward = raw_reward / self.reward_scale  # ì˜ˆ: reward_scale = 10.0
    return obs, normalized_reward, terminated, truncated, {"raw_reward": raw_reward}
    ```
  - PPOëŠ” `normalized_reward`ë¡œ í•™ìŠµ,  
    ë¡œê·¸/ë¶„ì„ ì‹œì—ëŠ” `info["raw_reward"]`ë¥¼ ì‚¬ìš©í•´ ì‹¤ì œ â€œë‹¹êµ¬ ì ìˆ˜ ê°ê°â€ ìœ ì§€.

- **ì¥ì **
  - ë³´ìƒ ë¶„í¬ë¥¼ ëª…í™•íˆ ì»¨íŠ¸ë¡¤ ê°€ëŠ¥.
  - SB3ì˜ VecNormalize ë‚´ë¶€ ë™ì‘ì— ì˜ì¡´í•˜ì§€ ì•Šê³ ,  
    ë…¼ë¬¸/ë¦¬í¬íŠ¸ì—ì„œ ìˆ˜ì‹ì„ ì§ì ‘ ëª…ì‹œí•˜ê¸° ì‰¬ì›€.

> í•„ìš”í•˜ë‹¤ë©´ ì´í›„ì— VecNormalizeë¥¼ ë‹¤ì‹œ ì¼œì„œ  
> **obs ì •ê·œí™” + reward ì¶”ê°€ ì •ê·œí™”**ë¥¼ ì¡°í•©í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥.

---

### 2. Stabilization Strategy

PPO í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ ì „ëµë“¤:

- **KL Divergence Monitoring**
  - `target_kl` ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ, í•œ ì—…ë°ì´íŠ¸ì—ì„œ KLì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ë©´  
    â†’ í•´ë‹¹ í•™ìŠµ ë£¨í”„ë¥¼ ì¡°ê¸° ì¢…ë£Œ(Early Stopping)  
    â†’ ì •ì±…ì´ ê°‘ìê¸° ë„ˆë¬´ ë§ì´ ë°”ë€ŒëŠ” ê²ƒì„ ë°©ì§€

- **LR Scheduling**
  - ì´ˆê¸°ì—ëŠ” ë¹„êµì  í° learning rateë¡œ íƒìƒ‰(exploration)ì„ ìœ ë„
  - ì´í›„ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ `lr â†’ decay`ì‹œí‚¤ë©´ì„œ  
    **ë¯¸ì„¸ ì¡°ì •(fine-tuning) êµ¬ê°„ì—ì„œì˜ ì§„ë™/ë°œì‚°**ì„ ì¤„ì„
  - ì˜ˆ: `linear_schedule(initial_lr)` í˜•íƒœì˜ ìŠ¤ì¼€ì¤„ ì‚¬ìš©

---

## Research / Analysis ë°©í–¥ (ì˜ˆì‹œ)

- **ì—°ì† ë“ì  ë¶„í¬ ë¶„ì„**
  - ì—í”¼ì†Œë“œ ë‹¹ ì—°ì† ë“ì  ìˆ˜(run length) íˆìŠ¤í† ê·¸ë¨
  - ì‚¬ëŒ ì„ ìˆ˜ì˜ í‰ê·  ì—°ì† ë“ì ê³¼ ë¹„êµ

- **ìƒ· ì„ íƒ íŒ¨í„´ ë¶„ì„**
  - í…œí”Œë¦¿ íŒ¨í„´(ë’¤ëŒ, ì•ëŒ, ì˜†ëŒ, ë¹—ê²¨, ëŒ€íšŒì „)ë³„ ì„±ê³µë¥  / ì„ íƒë¥ 
  - íŠ¹ì • ë°°ì¹˜ì— ëŒ€í•´ì„œ ì‚¬ëŒì´ ì„ íƒí•˜ëŠ” ë‘ê»˜/ë‹¹ì ê³¼ ëª¨ë¸ì˜ í–‰ë™ ë¹„êµ

- **Hyperparameter Study**
  - `Î³`, `Î»`, `clip_range`, `entropy_coef`, `lr` ë³€í™”ì— ë”°ë¥¸
  - ìˆ˜ë ´ ì†ë„ / ìµœì¢… ì„±ëŠ¥ / policy collapse ì—¬ë¶€ ë¹„êµ

---

## Troubleshooting Guide

### 1. PyGame ì°½ì´ ì•ˆ ëœ¨ê±°ë‚˜ ë°”ë¡œ êº¼ì§

- ì¦ìƒ:
  - `render_mode="human"` ìœ¼ë¡œ ì‹¤í–‰í–ˆëŠ”ë° ì°½ì´ ë°”ë¡œ êº¼ì§
- í™•ì¸ ì‚¬í•­:
  - WSL(Windows Subsystem for Linux) í™˜ê²½ì—ì„œëŠ” GUIê°€ ë°”ë¡œ ì•ˆ ëœ° ìˆ˜ ìˆìŒ â†’ Native Windows Python ê¶Œì¥
  - `render_mode=None` ìœ¼ë¡œ í•™ìŠµ, `render_mode="human"`ì€ ê´€ì „ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©

### 2. GPUë¥¼ ëª» ì°¾ëŠ” ê²½ìš°

- PyTorchì—ì„œ GPU ì¸ì‹ ì—¬ë¶€ í™•ì¸:

  ```python
  import torch
  print(torch.cuda.is_available())
  ```

- `False`ì¼ ê²½ìš°:
  - CUDA ë“œë¼ì´ë²„/Toolkit ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
  - Colab ì‚¬ìš© ì‹œ `ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU` ì„¤ì • í™•ì¸

### 3. í•™ìŠµì´ ì „í˜€ ì•ˆ ë˜ê³  Rewardê°€ ê³„ì† 0 ë˜ëŠ” ìŒìˆ˜ë§Œ ë‚˜ì˜¤ëŠ” ê²½ìš°

- ì ê²€ í¬ì¸íŠ¸:
  - ë³´ìƒ ì„¤ê³„ í™•ì¸:  
    - 3ì¿ ì…˜ ì„±ê³µ ì‹œ ë³´ìƒì´ ì‹¤ì œë¡œ ì–‘ìˆ˜ë¡œ ë“¤ì–´ì˜¤ëŠ”ì§€ (`raw_reward > 0`)  
    - miss ì‹œì—ë„ **ë„ˆë¬´ í° ìŒìˆ˜**ë¡œ íŒ¨ë„í‹°ë¥¼ ì£¼ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
  - action space í™•ì¸:
    - `Box(low=-1, high=1, shape=(n,))` í˜•íƒœì—ì„œ  
      ì‹¤ì œ ë¬¼ë¦¬ ë³€ìˆ˜(í˜, ê°ë„, ìŠ¤í•€)ë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œê°€ ì˜¬ë°”ë¥¸ì§€
  - ì´ˆê¸° í…œí”Œë¦¿:
    - 3ì¿ ì…˜ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ê°€ëŠ¥í•œ ë°°ì¹˜ì¸ì§€ (ë¶ˆê°€ëŠ¥í•œ ë°°ì¹˜ë©´ ì˜ì›íˆ 0ì )

### 4. Policy Collapse (ì„±ëŠ¥ì´ ê°‘ìê¸° ê¸‰ë½)

- ì¦ìƒ:
  - ì¼ì • ë‹¨ê³„ê¹Œì§€ í‰ê·  rewardê°€ ì˜¤ë¥´ë‹¤ê°€  
    ì–´ëŠ ìˆœê°„ë¶€í„° íŠ¹ì • í–‰ë™ì— ê³ ì •ë˜ê³ , ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§.
- ëŒ€ì‘:
  - **í•™ìŠµë¥  ë‚®ì¶”ê¸°**: ê¸°ì¡´ `3e-4` â†’ `3e-5` ë˜ëŠ” ë” ì‘ê²Œ
  - **checkpoint ë¡¤ë°±**:
    - `logs/best_model.zip` ë˜ëŠ” ì´ì „ checkpointë¥¼ ë¡œë“œ
    - ìœ„ì—ì„œ ì„¤ëª…í•œ Fine-tuning ë°©ì‹ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµ
  - **entropy_coef ì¦ê°€**:
    - íƒìƒ‰(Exploration)ì„ ì¡°ê¸ˆ ë” ìœ ì§€í•˜ë„ë¡ ì¡°ì •

---

## ğŸ“‚ Repository Structure (ì˜ˆì‹œ)

```text
billiard-rl-agent/
â”œâ”€â”€ environment_set.py     # BilliardEnv (Gym-style í™˜ê²½)
â”œâ”€â”€ simulation.py          # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ / Pymunk ê´€ë ¨ í•¨ìˆ˜
â”œâ”€â”€ play.py                # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì‹¤í—˜ìš©)
â”œâ”€â”€ utils/                 # ë¡œê¹…, ì‹œê°í™” ìœ í‹¸
â”œâ”€â”€ logs/                  # ëª¨ë¸ / tensorboard / ëª¨ë‹ˆí„° ë¡œê·¸
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“« Contact

- Author: (ì´ë¦„ ë˜ëŠ” GitHub ID)
- GitHub: [https://github.com/your-username/billiard-rl-agent](https://github.com/your-username/billiard-rl-agent)
- Issues / Pull Requests í™˜ì˜
